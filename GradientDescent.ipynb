{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0M8xPoyLthlN4G3JuMswu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"lPjsU15_1Wl4","executionInfo":{"status":"ok","timestamp":1710953225817,"user_tz":-120,"elapsed":28,"user":{"displayName":"Brian Mahlatse Malau","userId":"10164273695474635664"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","source":["np.dot(np.ones((4,8)).T,np.array([-2, 1 ,0.5 ,1]).reshape(-1,1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vS5Uzo5twK4h","executionInfo":{"status":"ok","timestamp":1710953475107,"user_tz":-120,"elapsed":725,"user":{"displayName":"Brian Mahlatse Malau","userId":"10164273695474635664"}},"outputId":"9b6a8155-b8e2-42bb-a45a-4b33ee3a28fe"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.5],\n","       [0.5],\n","       [0.5],\n","       [0.5],\n","       [0.5],\n","       [0.5],\n","       [0.5],\n","       [0.5]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["def calculate_weighted_sum(x, w, b):\n","    \"\"\"\n","    Compute the weighted sum of inputs.\n","\n","    Parameters:\n","    x (numpy.ndarray): Input data.\n","    w (numpy.ndarray): Weights matrix.\n","    b (numpy.ndarray): Bias vector.\n","\n","    Returns:\n","    numpy.ndarray or None: Weighted sum of inputs, or None if an error occurs.\n","    \"\"\"\n","    try:\n","        w = w.transpose()\n","        z = np.dot(w, x) + b\n","\n","        return z\n","    except Exception as e:\n","        # Handle exceptions, if any, and print an error message.\n","        print(\"An error occurred:\", e)\n","        return\n"],"metadata":{"id":"vnulPEek1vng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sigmoid(z):\n","    \"\"\"\n","    Compute the sigmoid function.\n","\n","    Parameters:\n","    z (numpy.ndarray): Input array.\n","\n","    Returns:\n","    numpy.ndarray: Output of the sigmoid function.\n","    \"\"\"\n","    try:\n","       return 1 / (1 + np.exp(-z))\n","    except Exception as e:\n","        # Handle exceptions, if any, and print an error message.\n","        print(\"An error occurred:\", e)\n","        return"],"metadata":{"id":"sJTSsr1w1zyC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sum_of_square(y, t):\n","    \"\"\"\n","    Calculate the sum of squares of differences between two arrays.\n","\n","    Parameters:\n","    y (array-like): Predicted values.\n","    t (array-like): Target or true values.\n","\n","    Returns:\n","    float: The sum of squares of differences.\n","    \"\"\"\n","    try:\n","        # Calculate the sum of squares of differences\n","        return 0.5 * np.sum((y - t) ** 2)\n","    except Exception as e:\n","        # Handle exceptions, if any, and print an error message.\n","        print(\"An error occurred:\", e)\n","        return None  # Return None if an error occurs"],"metadata":{"id":"g-jTZrr0liNO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def delta_n(a_n, t_n):\n","    \"\"\"\n","    Calculate the error (delta) for an output neuron in a neural network.\n","\n","    Parameters:\n","    a_n (float): Activation of the output neuron.\n","    t_n (float): Target or true value for the output neuron.\n","\n","    Returns:\n","    float: The calculated error (delta) for the output neuron.\n","    \"\"\"\n","    try:\n","        # Calculate the error (delta) using the derivative of the sigmoid function\n","        return (a_n - t_n) * a_n * (1 - a_n)\n","    except Exception as e:\n","        # Handle exceptions, if any, and print an error message.\n","        print(\"An error occurred:\", e)\n","        return None  # Return None if an error occurs"],"metadata":{"id":"njl5v07C136d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def delta_m(a_m, w, s_n):\n","    \"\"\"\n","    Calculate the error (delta) for a neuron in a hidden layer of a neural network.\n","\n","    Parameters:\n","    a_m (float): Activation of the neuron in the hidden layer.\n","    w (array-like): Weights connecting the hidden layer to the subsequent layer.\n","    s_n (array-like): Error (delta) at the subsequent layer.\n","\n","    Returns:\n","    float: The calculated error (delta) for the neuron in the hidden layer.\n","    \"\"\"\n","    try:\n","        # Calculate the error (delta) using the dot product of weights and subsequent layer error\n","        return np.dot(w, s_n) * a_m * (1 - a_m)\n","    except Exception as e:\n","        # Handle exceptions, if any, and print an error message.\n","        print(\"An error occurred:\", e)\n","        return None  # Return None if an error occurs"],"metadata":{"id":"FVA4gd2x19EI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def feedforward(input_x, weights, biases):\n","    \"\"\"\n","    Perform the feedforward process of a neural network.\n","\n","    Parameters:\n","    input_x (array-like): Input data to the neural network.\n","    weights (list of arrays): List of weight matrices for each layer.\n","    biases (list of arrays): List of bias vectors for each layer.\n","\n","    Returns:\n","    activation_values (list of arrays): List of activation values for each layer.\n","    \"\"\"\n","\n","    # Combine weights and biases for each layer\n","    weights_biases = zip(weights, biases)\n","\n","    # Initialize lists to store weighted sums and activation values\n","    weighted_sums = []\n","    activation_values = []\n","\n","    # Initialize the list of inputs with the initial input\n","    inputs = [input_x]\n","\n","    # Iterate over each layer's weights and biases\n","    for layer_weights, layer_biases in weights_biases:\n","        # Calculate the weighted sum for the current layer\n","        weighted_sum = calculate_weighted_sum(inputs[-1], layer_weights, layer_biases)\n","\n","        # Apply the sigmoid activation function to the weighted sum\n","        activation = sigmoid(weighted_sum)\n","\n","        # Append the activation values to the list\n","        activation_values.append(activation)\n","\n","        # Update the inputs list with the activation values for the next layer\n","        inputs.append(activation)\n","\n","    return activation_values\n"],"metadata":{"id":"e7QzSlbK2E24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def neural_training(input_x, t):\n","    \"\"\"\n","    Train a neural network using gradient descent.\n","\n","    Parameters:\n","    input_x (array-like): Input data to the neural network.\n","    t (array-like): Target or true values for the neural network.\n","\n","    Returns:\n","    list: List of costs (errors) at each iteration of training.\n","    \"\"\"\n","    input_x = input_x.reshape(-1, 1)\n","    t = t.reshape(-1, 1)\n","\n","    number_of_layers_excluding_input = 2\n","    number_of_nodes_per_layer = [4, 8, 3]\n","    learning_rate = 0.1\n","    # Initialize weights and biases\n","    weights = [np.ones([number_of_nodes_per_layer[i], number_of_nodes_per_layer[i + 1]]) for i in range(number_of_layers_excluding_input)]\n","    biases = [np.ones(num).reshape(-1, 1) for num in number_of_nodes_per_layer[1:]]\n","\n","    # Initialize the list to store costs\n","    cost = []\n","\n","    # Perform the feedforward pass\n","    output_activations = feedforward(input_x, weights, biases)\n","\n","    # Calculate and append the initial cost\n","    cost.append(sum_of_square(output_activations[-1], t))\n","\n","    # Initialize a list to store errors\n","    errors = []\n","    print('for sn a=',output_activations[-1])\n","    # Calculate the error for the output layer\n","    s_n = delta_n(output_activations[-1], t)\n","    errors.append(s_n)\n","\n","\n","    # Backpropagate the error through the network\n","    for (a_m, w_mn) in zip(output_activations[:-1][::-1], weights[::-1]):\n","        s_n = errors[-1]\n","        print(f'am={a_m}')\n","\n","        print(f'sn={s_n}')\n","        print('w',w_mn)\n","        s_m = delta_m(a_m, w_mn, s_n)\n","        errors.append(s_m)\n","\n","    # Prepare inputs for weight and bias update\n","    inputs = output_activations[:-1]\n","    inputs.insert(0, input_x)\n","\n","    # Update weights and biases using gradient descent\n","    for (i, (a_m, s_n)) in enumerate(zip(inputs, errors[::-1])):\n","        s_n=s_n.reshape(-1,1).T\n","        a_m=a_m.reshape(-1,1)\n","        # if i==0:\n","        #     print(f'a{i}={a_m}')\n","\n","        #     print(f's{i}={s_n}')\n","\n","        weights[i] -= learning_rate * s_n * a_m\n","\n","        biases_update = (learning_rate * s_n).reshape(-1, 1)\n","        biases[i] -= biases_update\n","\n","    # Perform another feedforward pass to calculate the updated cost\n","    output_activations = feedforward(input_x, weights, biases)\n","    cost.append(sum_of_square(output_activations[-1], t))\n","\n","    return cost\n"],"metadata":{"id":"roT2okZ82J0F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Read input and target values\n","    values = list(map(float, input().split()))\n","    inputs = np.array(values[:4])\n","    targets = np.array(values[4:])\n","\n","    cost=neural_training(inputs,targets)\n","    for loss in cost:\n","        print(f'{loss: .4f}')\n","    #1.4 0 -2.5 -3 0.4 0.6 0\n","    #-2 1 0.5 -1 0 0 1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6axGWLh0dD9l","executionInfo":{"status":"ok","timestamp":1710933637413,"user_tz":-120,"elapsed":7006,"user":{"displayName":"Brian Mahlatse Malau","userId":"10164273695474635664"}},"outputId":"f45f01cb-e515-4250-e8cf-ef81da21f0c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-2 1 0.5 -1 0 0 1\n","for sn a= [[0.9823693]\n"," [0.9823693]\n"," [0.9823693]]\n","am=[[0.37754067]\n"," [0.37754067]\n"," [0.37754067]\n"," [0.37754067]\n"," [0.37754067]\n"," [0.37754067]\n"," [0.37754067]\n"," [0.37754067]]\n","sn=[[ 0.0170145 ]\n"," [ 0.0170145 ]\n"," [-0.00030536]]\n","w [[1. 1. 1.]\n"," [1. 1. 1.]\n"," [1. 1. 1.]\n"," [1. 1. 1.]\n"," [1. 1. 1.]\n"," [1. 1. 1.]\n"," [1. 1. 1.]\n"," [1. 1. 1.]]\n"," 0.9652\n"," 0.9647\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"y7IRBabfWTns"},"execution_count":null,"outputs":[]}]}